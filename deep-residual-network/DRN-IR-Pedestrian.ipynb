{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "#%% Device config\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Dataset\n",
    "def read_images(path, num_img):\n",
    "    array = np.zeros([num_img,64*32])\n",
    "    i = 0\n",
    "    for img in os.listdir(path):\n",
    "        img_path = path + \"\\\\\" + img\n",
    "        img = Image.open(img_path, mode = 'r')\n",
    "        data = np.asarray(img,dtype = \"uint8\")\n",
    "        data = data.flatten()\n",
    "        array[i,:] = data\n",
    "        i += 1      \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_negative_tensor:  torch.Size([42000, 2048])\n",
      "y_train_negative_tensor:  torch.Size([42000])\n"
     ]
    }
   ],
   "source": [
    "# read train negative  43390\n",
    "train_negative_path = r\"LSIFIR\\Classification\\Train\\neg\"\n",
    "num_train_negative_img = 43390\n",
    "train_negative_array = read_images(train_negative_path,num_train_negative_img)\n",
    "x_train_negative_tensor = torch.from_numpy(train_negative_array[:42000,:])\n",
    "print(\"x_train_negative_tensor: \",x_train_negative_tensor.size())\n",
    "y_train_negative_tensor = torch.zeros(42000,dtype = torch.long)\n",
    "print(\"y_train_negative_tensor: \",y_train_negative_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_positive_tensor:  torch.Size([10000, 2048])\n",
      "y_train_positive_tensor:  torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# read train positive 10208\n",
    "train_positive_path = r\"LSIFIR\\Classification\\Train\\pos\"\n",
    "num_train_positive_img = 10208\n",
    "train_positive_array = read_images(train_positive_path,num_train_positive_img)\n",
    "x_train_positive_tensor = torch.from_numpy(train_positive_array[:10000,:])\n",
    "print(\"x_train_positive_tensor: \",x_train_positive_tensor.size())\n",
    "y_train_positive_tensor = torch.ones(10000,dtype = torch.long)\n",
    "print(\"y_train_positive_tensor: \",y_train_positive_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  torch.Size([52000, 2048])\n",
      "y_train:  torch.Size([52000])\n"
     ]
    }
   ],
   "source": [
    "# concat train\n",
    "x_train = torch.cat((x_train_negative_tensor, x_train_positive_tensor), 0)\n",
    "y_train = torch.cat((y_train_negative_tensor, y_train_positive_tensor), 0)\n",
    "print(\"x_train: \",x_train.size())\n",
    "print(\"y_train: \",y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test_negative_tensor:  torch.Size([18056, 2048])\n",
      "y_test_negative_tensor:  torch.Size([18056])\n"
     ]
    }
   ],
   "source": [
    "# read test negative  22050\n",
    "test_negative_path = r\"LSIFIR\\Classification\\Test\\neg\"\n",
    "num_test_negative_img = 22050\n",
    "test_negative_array = read_images(test_negative_path,num_test_negative_img)\n",
    "x_test_negative_tensor = torch.from_numpy(test_negative_array[:18056,:])\n",
    "print(\"x_test_negative_tensor: \",x_test_negative_tensor.size())\n",
    "y_test_negative_tensor = torch.zeros(18056,dtype = torch.long)\n",
    "print(\"y_test_negative_tensor: \",y_test_negative_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test_positive_tensor:  torch.Size([5944, 2048])\n",
      "y_test_positive_tensor:  torch.Size([5944])\n"
     ]
    }
   ],
   "source": [
    "# read test positive 5944\n",
    "test_positive_path = r\"LSIFIR\\Classification\\Test\\pos\"\n",
    "num_test_positive_img = 5944\n",
    "test_positive_array = read_images(test_positive_path,num_test_positive_img)\n",
    "x_test_positive_tensor = torch.from_numpy(test_positive_array)\n",
    "print(\"x_test_positive_tensor: \",x_test_positive_tensor.size())\n",
    "y_test_positive_tensor = torch.zeros(num_test_positive_img,dtype = torch.long)\n",
    "print(\"y_test_positive_tensor: \",y_test_positive_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test:  torch.Size([24000, 2048])\n",
      "y_test:  torch.Size([24000])\n"
     ]
    }
   ],
   "source": [
    "# concat test\n",
    "x_test = torch.cat((x_test_negative_tensor, x_test_positive_tensor), 0)\n",
    "y_test = torch.cat((y_test_negative_tensor, y_test_positive_tensor), 0)\n",
    "print(\"x_test: \",x_test.size())\n",
    "print(\"y_test: \",y_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x156380a9880>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize\n",
    "plt.imshow(x_train[45001,:].reshape(64,32), cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "num_classes = 2\n",
    "# Hyper parameters\n",
    "num_epochs = 100\n",
    "batch_size = 2000\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(x_train,y_train)\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.utils.data.TensorDataset(x_test,y_test)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride = 1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size = 3, stride = stride, padding = 1, bias = False)\n",
    "        \n",
    "def conv1x1(in_planes, out_planes, stride = 1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size = 1, stride = stride, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self,inplanes, planes, stride = 1, downsample = None):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.drop = nn.Dropout(0.9)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.drop(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, block, layers, num_classes = num_classes):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride = 2, padding = 3, bias= False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size= 3, stride = 2, padding = 1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "    \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(256*block.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode = \"fan_out\", nonlinearity = \"relu\") # çok büyük ya da çok küçük sayıları değil 0'a yakın sayıları weightlere atıyor\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "                 \n",
    "    def _make_layer(self, block, planes, blocks, stride = 1): # planes = input channel, blocks = basic block count\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes*block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                    conv1x1(self.inplanes, planes*block.expansion, stride),\n",
    "                    nn.BatchNorm2d(planes*block.expansion))\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes*block.expansion\n",
    "        for _ in range(1,blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(BasicBlock, [2,2,2])\n",
    "\n",
    "# model = ResNet(BasicBlock, [2,2,2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% train\n",
    "\n",
    "loss_list = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "use_gpu = False\n",
    "\n",
    "total_step = len(trainloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        images = images.view(batch_size,1,64,32)\n",
    "        images = images.float()\n",
    "        \n",
    "        # gpu\n",
    "        if use_gpu:\n",
    "            if torch.cuda.is_available():\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward and optimization\n",
    "        optimizer.zero_grad() # gradietler sıfırlandı\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            print(\"epoch: {} {}/{}\".format(epoch,i,total_step))\n",
    "\n",
    "    # train\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): # update yok\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "            images = images.view(batch_size,1,64,32)\n",
    "            images = images.float()\n",
    "            \n",
    "            # gpu\n",
    "            if use_gpu:\n",
    "                if torch.cuda.is_available():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(\"Accuracy train %d %%\"%(100*correct/total))\n",
    "    train_acc.append(100*correct/total)\n",
    "\n",
    "    # test\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): \n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.view(batch_size,1,64,32)\n",
    "            images = images.float()\n",
    "            \n",
    "            # gpu\n",
    "            if use_gpu:\n",
    "                if torch.cuda.is_available():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(\"Accuracy test %d %%\"%(100*correct/total))\n",
    "    train_acc.append(100*correct/total)\n",
    "\n",
    "    loss_list.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% visualize\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.plot(loss_list,label = \"Loss\",color = \"black\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.array(test_acc)/100,label = \"Test Acc\",color=\"green\")\n",
    "ax2.plot(np.array(train_acc)/100,label = \"Train Acc\",color= \"red\")\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax1.set_xlabel('Epoch')\n",
    "fig.tight_layout()\n",
    "plt.title(\"Loss vs Test Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bizim örneğimizde image shape 64x32. Padding değeri girmiyoruz, varsayılan değerini alıyor ki o da 0'a eşit. Stride değeri girmiyoruz, varsayılan değeri 1. Dilation değeri girmiyoruz, varsayılan değeri 1. Bu bilgileri toparladığımızda;\n",
    "\n",
    "o = output = bilinmeyenimiz\n",
    "\n",
    "p = padding = 0\n",
    "\n",
    "k = kernel_size = 5\n",
    "\n",
    "s = stride = 1\n",
    "\n",
    "d = dilation = 1\n",
    "\n",
    "i =input shape = 64x32\n",
    "\n",
    "Bir Conv2d metoduna giren bir input aşağıdaki formüle göre çıkış boyutu verir.\n",
    "\n",
    "o = [i + 2*p - k - (k-1)*(d-1)]/s + 1\n",
    "Kendi parametre değerlerimizle çözüm yaptığımızda;\n",
    "\n",
    "i=64 için o=60,\n",
    "\n",
    "i=32 için o=28 çıkar. Böylece inputumuz ilk Conv2d metodundan çıktığında 60x28 shape'ini alıyor.\n",
    "\n",
    "Ardından MaxPooling2d yapıyoruz. Örneğimizde pool_size=(2,2) belirlediğimiz için 60x28 shape'i 30x14'e indiriyoruz.\n",
    "\n",
    "Ardından 30x14 için tekrar Conv2d uyguluyoruz.\n",
    "\n",
    "i=30 için o=26,\n",
    "\n",
    "i=14 için o=10, böylece inputumuz ikinci Conv2d metodundan çıktığında 26x10 shape'ini alıyor.\n",
    "\n",
    "Ardından tekrar MaxPooling2d yapıyoruz. Örneğimizde pool_size=(2,2) belirlediğimiz için 26x10 shape'i 13x5'e indiriyoruz.\n",
    "\n",
    "Son Conv2d metodunda feature map sayısını 16 olarak belirlediğimiz için elimizde 16 adet 13x5 image oluyor. Bu yüzden nn.Linear(16*13*5,520) yazıyoruz. Umarım faydalı olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% IMAGE TO TEST\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "#image objesi olarak im değişkenine kayıt edilir.\n",
    "im = Image.open('sekiz.jpg')\n",
    " \n",
    "#image'ı numpy array'e dönüştürür\n",
    "im2arr = np.array(im)\n",
    " \n",
    "#image, im2arr içinde 28x28x3 formatında. Sondaki 3, channel değerini ifade ediyor. Channel=3 renkli yani r,g,b değerleri var anlamında. Biz modelimizi channel=1 yani gri'ye göre eğittik ve input_shape parametresini ona göre belirledik. Bu yüzden griye dönüştürerek 28x28x1 şekline getirmemiz gerekiyor.\n",
    " \n",
    "def rgb2gray(rgb):\n",
    "#image gray transform matris ile dot product yapıyor ve ch=1 yani 28x28 yani gri forma dönüşüyor. Fakat bize 28x28x1 şekli lazım. Predict öncesinde onu reshape yapacağız.\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    " \n",
    "gray = rgb2gray(im2arr)\n",
    " \n",
    "#gri image array'ından image objesi oluşturuluyor.\n",
    "arr2im = Image.fromarray(gray)\n",
    " \n",
    "#image plot ediliyor.\n",
    "plt.imshow(arr2im)\n",
    "plt.axis(\"off\")\n",
    " \n",
    "#yüklenen image'ın piksel değerleri normalize ediliyor ve predict için reshape yapılıyor\n",
    "gray_norm = gray/255.0\n",
    "gray_norm = gray_norm.reshape(28,28,1)\n",
    " \n",
    "#%% TEST\n",
    " \n",
    "#predict ve predictin olasılık değeri değişkenlere kaydediliyor.\n",
    "y_head = model.predict_classes([[gray_norm]])\n",
    "y_proba = model.predict_proba([[gray_norm]])\n",
    " \n",
    "print(\"% {0:.2f} ihtimalle: \".format((y_proba[0,y_head[0]])*100), y_head[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import pickle\n",
    " \n",
    "filenames = os.listdir(\"./train/malignant\") # resimlerin bulunduğu klasör\n",
    "images = []\n",
    "i = 0\n",
    " \n",
    "for filename in filenames:\n",
    "\ti += 1\n",
    "\timg = cv2.imread(\"./train/malignant/\"+ filename)\n",
    "\timages.append(img)\n",
    "\tif i%100 == 0:\n",
    "\t\tprint(\"{} Resim Yüklendi\".format(i))\n",
    " \n",
    "df = pd.DataFrame({\n",
    "    'image': images\n",
    "})\n",
    " \n",
    "pickle_out = open(\"malignant_train.pickle\",\"wb\")\n",
    "pickle.dump(df, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pd.read_pickle('malignant_train.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
